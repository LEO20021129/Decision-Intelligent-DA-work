{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbed9e60-58d1-4144-9f5b-9ca3ca725173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 07:51:43 - INFO - Using Windows Integrated Authentication (Trusted_Connection=yes).\n",
      "2025-06-06 07:51:43 - INFO - Connected to 'RLS162Demo'. Server version: Microsoft SQL Server 2022 (RTM-GDR) (KB5046861) - 16.0.1135.2 (X64) \n",
      "2025-06-06 07:51:43 - INFO - Extracting 'SLS3.[Customer]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'SLS3.[Invoice]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'SLS3.[InvoiceItem]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'SLS3.[Product]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'HCM3.[Employee]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'LGS3.[InventoryVoucher]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracting 'RPA3.[ReceivableNote]' …\n",
      "2025-06-06 07:51:43 - INFO - Extracted all seven tables.\n",
      "2025-06-06 07:51:43 - INFO - SalesFact transformation complete (rows = 1).\n",
      "2025-06-06 07:51:43 - INFO - Loading SalesFact into dbo.SalesFact (replace if exists)…\n",
      "2025-06-06 07:51:44 - INFO - dbo.SalesFact loaded (rows = 1).\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Customer.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Invoice.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Invoiceitem.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Product.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Employee.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Inventoryvoucher.\n",
      "2025-06-06 07:51:44 - INFO - Saved snapshot for Receivablenote.\n",
      "2025-06-06 07:51:44 - INFO - Saved SalesFact snapshots.\n",
      "2025-06-06 07:51:44 - INFO - === ETL PIPELINE COMPLETED SUCCESSFULLY ===\n",
      "2025-06-06 07:51:44 - INFO - Disposed engine.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine.url import URL\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) USER CONFIGURATION: set these to your environment values\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "SQL_SERVER_HOST     = \"WENLINHNBB\"   # ← your server or instance (e.g. \"localhost\\\\SQLEXPRESS\")\n",
    "SQL_SERVER_PORT     = 1433           # ← usually 1433\n",
    "SQL_SERVER_DATABASE = \"RLS162Demo\"   # ← your database name\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Configure logging\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_sqlalchemy_engine():\n",
    "    if SQL_SERVER_HOST in (\"\", \"YOUR_SQL_SERVER_HOST_HERE\"):\n",
    "        raise ValueError(\"Set SQL_SERVER_HOST to your actual server/instance.\")\n",
    "    if SQL_SERVER_DATABASE in (\"\", \"YOUR_DATABASE_NAME_HERE\"):\n",
    "        raise ValueError(\"Set SQL_SERVER_DATABASE to your actual database name.\")\n",
    "\n",
    "    driver_params = {\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\",\n",
    "        \"Trusted_Connection\": \"yes\"\n",
    "    }\n",
    "    logger.info(\"Using Windows Integrated Authentication (Trusted_Connection=yes).\")\n",
    "    connection_url = URL.create(\n",
    "        \"mssql+pyodbc\",\n",
    "        username=None,\n",
    "        password=None,\n",
    "        host=SQL_SERVER_HOST,\n",
    "        port=int(SQL_SERVER_PORT),\n",
    "        database=SQL_SERVER_DATABASE,\n",
    "        query=driver_params,\n",
    "    )\n",
    "    try:\n",
    "        engine = create_engine(connection_url, fast_executemany=True, echo=False)\n",
    "        with engine.connect() as conn:\n",
    "            version = conn.execute(text(\"SELECT @@VERSION\")).scalar()\n",
    "            logger.info(f\"Connected to '{SQL_SERVER_DATABASE}'. Server version: {version.splitlines()[0]}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        raise ConnectionError(f\"Could not create SQLAlchemy engine: {e}\")\n",
    "\n",
    "def extract_all_tables(engine):\n",
    "    table_map = {\n",
    "        \"customer\":         (\"SLS3\", \"Customer\"),\n",
    "        \"invoice\":          (\"SLS3\", \"Invoice\"),\n",
    "        \"invoiceitem\":      (\"SLS3\", \"InvoiceItem\"),\n",
    "        \"product\":          (\"SLS3\", \"Product\"),\n",
    "        \"employee\":         (\"HCM3\", \"Employee\"),\n",
    "        \"inventoryvoucher\": (\"LGS3\", \"InventoryVoucher\"),\n",
    "        \"receivablenote\":   (\"RPA3\", \"ReceivableNote\")\n",
    "    }\n",
    "    dfs = {}\n",
    "    for key, (schema, tbl) in table_map.items():\n",
    "        fq = f\"{schema}.[{tbl}]\"\n",
    "        logger.info(f\"Extracting '{fq}' …\")\n",
    "        try:\n",
    "            dfs[key] = pd.read_sql(f\"SELECT * FROM {fq}\", engine)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract {fq}: {e}\")\n",
    "    logger.info(\"Extracted all seven tables.\")\n",
    "    return dfs\n",
    "\n",
    "def transform_sales_fact(dfs):\n",
    "    cust_df  = dfs[\"customer\"]\n",
    "    inv_df   = dfs[\"invoice\"]\n",
    "    items_df = dfs[\"invoiceitem\"]\n",
    "    prod_df  = dfs[\"product\"]\n",
    "\n",
    "    # Merge InvoiceItem → Invoice on InvoiceRef → InvoiceID\n",
    "    merged = items_df.merge(\n",
    "        inv_df[[\"InvoiceID\", \"Date\", \"CustomerRef\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"InvoiceRef\",\n",
    "        right_on=\"InvoiceID\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "    # Merge Customer on CustomerRef → CustomerID, pulling Number, Type, CurrencyRef\n",
    "    merged = merged.merge(\n",
    "        cust_df[[\"CustomerID\", \"Number\", \"Type\", \"CurrencyRef\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"CustomerRef\",\n",
    "        right_on=\"CustomerID\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "    # Merge Product on ProductRef → ProductID, pulling Name, Number, UnitRef, PriceBaseUnitRef\n",
    "    merged = merged.merge(\n",
    "        prod_df[[\"ProductID\", \"Name\", \"Number\", \"UnitRef\", \"PriceBaseUnitRef\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"ProductRef\",\n",
    "        right_on=\"ProductID\",\n",
    "        validate=\"many_to_one\"\n",
    "    )\n",
    "\n",
    "    # Rename columns: Date→InvoiceDate, Number_x→CustomerNumber, Number_y→ProductNumber, Name→ProductName\n",
    "    salesfact = merged.rename(columns={\n",
    "        \"Date\": \"InvoiceDate\",\n",
    "        \"Number_x\": \"CustomerNumber\",\n",
    "        \"Number_y\": \"ProductNumber\",\n",
    "        \"Name\": \"ProductName\"\n",
    "    })\n",
    "\n",
    "    # Coerce numeric columns if present\n",
    "    for col in (\"Quantity\", \"Price\", \"NetPrice\"):\n",
    "        if col in salesfact.columns:\n",
    "            salesfact[col] = pd.to_numeric(salesfact[col], errors=\"coerce\")\n",
    "\n",
    "    logger.info(f\"SalesFact transformation complete (rows = {len(salesfact):,}).\")\n",
    "    return salesfact\n",
    "\n",
    "def load_salesfact_to_sql(df, engine):\n",
    "    try:\n",
    "        logger.info(\"Loading SalesFact into dbo.SalesFact (replace if exists)…\")\n",
    "        df.to_sql(\n",
    "            name=\"SalesFact\",\n",
    "            schema=\"dbo\",\n",
    "            con=engine,\n",
    "            if_exists=\"replace\",\n",
    "            index=False,\n",
    "            method=\"multi\",\n",
    "            chunksize=5000\n",
    "        )\n",
    "        logger.info(f\"dbo.SalesFact loaded (rows = {len(df):,}).\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load dbo.SalesFact: {e}\")\n",
    "\n",
    "def save_all_snapshots(dfs, salesfact_df):\n",
    "    try:\n",
    "        for key, df in dfs.items():\n",
    "            name = key[0].upper() + key[1:]\n",
    "            df.to_csv(f\"{name}.csv\", index=False)\n",
    "            df.to_parquet(f\"{name}.parquet\", index=False)\n",
    "            logger.info(f\"Saved snapshot for {name}.\")\n",
    "        salesfact_df.to_csv(\"SalesFact_snapshot.csv\", index=False)\n",
    "        salesfact_df.to_parquet(\"SalesFact_snapshot.parquet\", index=False)\n",
    "        logger.info(\"Saved SalesFact snapshots.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save snapshots: {e}\")\n",
    "\n",
    "def run_full_etl():\n",
    "    engine = None\n",
    "    try:\n",
    "        engine = get_sqlalchemy_engine()\n",
    "        dfs = extract_all_tables(engine)\n",
    "        salesfact_df = transform_sales_fact(dfs)\n",
    "        load_salesfact_to_sql(salesfact_df, engine)\n",
    "        save_all_snapshots(dfs, salesfact_df)\n",
    "        logger.info(\"=== ETL PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "    except Exception as exc:\n",
    "        logger.error(f\"ETL aborted due to exception:\\n{exc}\")\n",
    "    finally:\n",
    "        if engine is not None:\n",
    "            try:\n",
    "                engine.dispose()\n",
    "                logger.info(\"Disposed engine.\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Execute the ETL pipeline\n",
    "run_full_etl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c8749-7235-4c75-a6f4-c40adc04cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f69f12-8ddd-42f7-bce3-4c06f6ed7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f9eb4-e154-4813-9456-2655cbc0e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the sales_fact DataFrame\n",
    "# For example, you can load it from a CSV file or create it from scratch\n",
    "import pandas as pd\n",
    "\n",
    "# Option 1: Load from a file\n",
    "# sales_fact = pd.read_csv('your_sales_data.csv')\n",
    "\n",
    "# Option 2: Create a sample DataFrame for testing\n",
    "sales_fact = pd.DataFrame({\n",
    "    'invoice_date': pd.date_range(start='2023-01-01', periods=10),\n",
    "    'net_line_price': [100, 150, 200, 120, 180, 250, 300, 220, 190, 210]\n",
    "})\n",
    "\n",
    "# Now the original code will work\n",
    "# 1) Group \"SalesFact\" by date (day‐level) and sum net_line_price\n",
    "daily = (\n",
    "    sales_fact\n",
    "    .groupby(sales_fact[\"invoice_date\"].dt.floor(\"D\"))[\"net_line_price\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"invoice_date\": \"ds\", \"net_line_price\": \"y\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8a47f-0a99-4117-9b53-dcc62842bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(\n",
    "    growth=\"linear\",\n",
    "    daily_seasonality=False,  # turn off auto daily, since we only have daily\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode=\"additive\"\n",
    ")\n",
    "m.fit(daily)\n",
    "\n",
    "# Build a DataFrame of future dates: next 30 days (or project range you like)\n",
    "future = m.make_future_dataframe(periods=30, freq=\"D\")\n",
    "\n",
    "# Forecast\n",
    "forecast = m.predict(future)\n",
    "# forecast contains: ds, yhat, yhat_lower, yhat_upper, plus trend and seasonality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749576c-1999-4000-abbe-e724da956c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1.4 Compute residuals & save\n",
    "# ------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "# Redefine OUTPUT_FOLDER here (exactly as in your config section):\n",
    "OUTPUT_FOLDER = r\"E:\\decision_intelligent\\task2\\output_files\\RLS162Demo\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)  # This will create all intermediate directories as needed\n",
    "\n",
    "# 'daily' and 'forecast' must already exist from the previous steps.\n",
    "\n",
    "# 1) Compute residuals: actual minus predicted, only for dates that exist in 'daily'\n",
    "hist = forecast.merge(daily, on=\"ds\", how=\"left\")\n",
    "hist[\"residual\"] = hist[\"y\"] - hist[\"yhat\"]\n",
    "\n",
    "# 2) Save the residual file\n",
    "residuals_path = os.path.join(OUTPUT_FOLDER, \"sales_trend_residuals.csv\")\n",
    "hist[[\"ds\", \"y\", \"yhat\", \"residual\"]].to_csv(residuals_path, index=False)\n",
    "\n",
    "logger.info(f\"Saved daily residuals CSV to: {residuals_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
